{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the connection Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a SparkSession with specific configurations\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark Application\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"4\") \\\n",
    "    .config(\"spark.python.worker.timeout\", \"600\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_location = \"/home/jovyan/work/adult-preprocessed.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = spark.read.format('csv') \\\n",
    "               .option('inferSchema', True) \\\n",
    "               .option('header', False) \\\n",
    "               .option('sep', ',') \\\n",
    "               .load(dataset_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = dataset \\\n",
    "            .withColumnRenamed('_c0', 'idade') \\\n",
    "            .withColumnRenamed('_c1', 'classe_trabalho') \\\n",
    "            .withColumnRenamed('_c2', 'final_weight') \\\n",
    "            .withColumnRenamed('_c3', 'escolaridade') \\\n",
    "            .withColumnRenamed('_c4', 'escolaridade_num') \\\n",
    "            .withColumnRenamed('_c5', 'estado_civil') \\\n",
    "            .withColumnRenamed('_c6', 'ocupacao') \\\n",
    "            .withColumnRenamed('_c7', 'relacionamento_householder') \\\n",
    "            .withColumnRenamed('_c8', 'raca') \\\n",
    "            .withColumnRenamed('_c9', 'sexo') \\\n",
    "            .withColumnRenamed('_c10', 'ganho_capital') \\\n",
    "            .withColumnRenamed('_c11', 'perda_capital') \\\n",
    "            .withColumnRenamed('_c12', 'jornada_trabalho') \\\n",
    "            .withColumnRenamed('_c13', 'nacionalidade') \\\n",
    "            .withColumnRenamed('_c14', 'renda_anual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "dataset1 = dataset1.withColumn(\"classe_trabalho\", col(\"classe_trabalho\").cast(\"string\"))\\\n",
    "        .withColumn(\"escolaridade\", col(\"escolaridade\").cast(\"string\"))\\\n",
    "        .withColumn(\"estado_civil\", col(\"estado_civil\").cast(\"string\"))\\\n",
    "        .withColumn(\"ocupacao\", col(\"ocupacao\").cast(\"string\"))\\\n",
    "        .withColumn(\"relacionamento_householder\", col(\"relacionamento_householder\").cast(\"string\"))\\\n",
    "        .withColumn(\"raca\", col(\"raca\").cast(\"string\"))\\\n",
    "        .withColumn(\"sexo\", col(\"sexo\").cast(\"string\"))\\\n",
    "        .withColumn(\"nacionalidade\", col(\"nacionalidade\").cast(\"string\"))\\\n",
    "        .withColumn(\"renda_anual\", col(\"renda_anual\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenha o valor máximo de capital líquido (ganho de capital - perda de capital)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+\n",
      "|max((ganho_capital - perda_capital))|\n",
      "+------------------------------------+\n",
      "|                               99999|\n",
      "+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1.selectExpr(\"max(ganho_capital - perda_capital)\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenha a idade média das pessoas viúvas com jornada de trabalho acima de 20 horas semanais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|        avg(idade)|\n",
      "+------------------+\n",
      "|56.394101876675606|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 \\\n",
    "    .filter(\"jornada_trabalho > 20\") \\\n",
    "    .filter(dataset1[\"estado_civil\"] == \"Widowed\") \\\n",
    "    .selectExpr(\"avg(idade)\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtendo valores distintos para a combinação de sexo e raça para pessoas com idade acima de 60 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------+\n",
      "|  sexo|              raca|\n",
      "+------+------------------+\n",
      "|  Male|             White|\n",
      "|Female|Asian-Pac-Islander|\n",
      "|Female|             White|\n",
      "|Female|Amer-Indian-Eskimo|\n",
      "|  Male|             Other|\n",
      "|  Male|             Black|\n",
      "|  Male|Asian-Pac-Islander|\n",
      "|  Male|Amer-Indian-Eskimo|\n",
      "|Female|             Other|\n",
      "|Female|             Black|\n",
      "+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 \\\n",
    "    .filter(\"idade > 60\") \\\n",
    "    .select(\"sexo\", \"raca\") \\\n",
    "    .distinct() \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenha a média de capital líquido (ganho de capital - perda de capital) por escolaridade de pessoas com idade acima de 30 anos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------------------------+\n",
      "|escolaridade|avg((ganho_capital - perda_capital))|\n",
      "+------------+------------------------------------+\n",
      "|     Masters|                  2643.3674540682414|\n",
      "|        10th|                   533.1254612546126|\n",
      "|     5th-6th|                  132.26141078838174|\n",
      "|  Assoc-acdm|                   732.9663072776281|\n",
      "|   Assoc-voc|                   655.4662638469285|\n",
      "|     7th-8th|                  196.75375939849624|\n",
      "|         9th|                   430.9438202247191|\n",
      "|     HS-grad|                   638.0167474048443|\n",
      "|   Bachelors|                  2117.6493677555322|\n",
      "|        11th|                   262.4105461393597|\n",
      "|     1st-4th|                   94.40579710144928|\n",
      "|   Preschool|                   72.71794871794872|\n",
      "|        12th|                  468.90909090909093|\n",
      "|   Doctorate|                   4751.202046035805|\n",
      "|Some-college|                   776.4566966398486|\n",
      "| Prof-school|                  10928.190291262135|\n",
      "+------------+------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import avg, expr\n",
    "\n",
    "dataset1 \\\n",
    "    .filter('idade > 30') \\\n",
    "    .groupBy('escolaridade') \\\n",
    "    .agg(avg(expr('ganho_capital - perda_capital'))) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obtenha a combinação de escolaridade e ocupação com menor jornada de trabalho média que tenham renda maior de 50 mil dólares por ano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-------------+\n",
      "|escolaridade|     ocupacao|jornada_media|\n",
      "+------------+-------------+-------------+\n",
      "|     Masters|Other-service|         15.0|\n",
      "+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1 \\\n",
    "    .filter(dataset1['renda_anual'] ==  '>50K') \\\n",
    "    .groupBy('escolaridade', 'ocupacao') \\\n",
    "    .agg(avg('jornada_trabalho').alias('jornada_media')) \\\n",
    "    .orderBy('jornada_media') \\\n",
    "    .limit(1) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### O atributo final_weight representa quantas vezes uma determinada leitura do censo é repetida, ou seja, o peso daquela linha no conjunto de dados. Itere sobre a lista das 5 maiores nacionalidades (baseado na soma do final weight) e para cada uma, obtenha os três estados civis com maior jornada de trabalho médio. Utilize a API SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1.createOrReplaceTempView(\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nacionalidade: United-States\n",
      "Married-civ-spouse: 43.391457211250746\n",
      "Divorced: 41.30946660259491\n",
      "Married-AF-spouse: 41.130434782608695\n",
      "--------------------------------------------------\n",
      "nacionalidade: Mexico\n",
      "Divorced: 42.45454545454545\n",
      "Married-civ-spouse: 41.48874598070739\n",
      "Separated: 39.75757575757576\n",
      "--------------------------------------------------\n",
      "nacionalidade: ?\n",
      "Married-spouse-absent: 43.80952380952381\n",
      "Married-civ-spouse: 43.429577464788736\n",
      "Widowed: 42.92857142857143\n",
      "--------------------------------------------------\n",
      "nacionalidade: Philippines\n",
      "Separated: 43.25\n",
      "Married-civ-spouse: 40.922330097087375\n",
      "Married-spouse-absent: 39.45454545454545\n",
      "--------------------------------------------------\n",
      "nacionalidade: El-Salvador\n",
      "Separated: 38.0\n",
      "Married-civ-spouse: 37.67567567567568\n",
      "Divorced: 37.25\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "nacionalidades = spark.sql('''\n",
    "    select nacionalidade, sum(final_weight) qtde\n",
    "    from dataset\n",
    "    group by nacionalidade\n",
    "    order by qtde desc\n",
    "    limit 5\n",
    "''').collect()\n",
    "\n",
    "for row in nacionalidades:\n",
    "    nacionalidade = row[\"nacionalidade\"]\n",
    "    print(f\"nacionalidade: {nacionalidade}\")\n",
    "\n",
    "    res = spark.sql('''\n",
    "        select estado_civil, avg(jornada_trabalho) as jornada_media\n",
    "        from dataset\n",
    "        where nacionalidade = '%s'\n",
    "        group by estado_civil\n",
    "        order by jornada_media desc\n",
    "        limit 3\n",
    "    ''' % nacionalidade).collect()\n",
    "\n",
    "    for row2 in res:\n",
    "        print(f\"{row2['estado_civil']}: {row2['jornada_media']}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('idade', 'int')]\n",
      "0\n",
      "--------------------\n",
      "[('classe_trabalho', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('final_weight', 'int')]\n",
      "0\n",
      "--------------------\n",
      "[('escolaridade', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('escolaridade_num', 'int')]\n",
      "0\n",
      "--------------------\n",
      "[('estado_civil', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('ocupacao', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('relacionamento_householder', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('raca', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('sexo', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('ganho_capital', 'int')]\n",
      "0\n",
      "--------------------\n",
      "[('perda_capital', 'int')]\n",
      "0\n",
      "--------------------\n",
      "[('jornada_trabalho', 'int')]\n",
      "0\n",
      "--------------------\n",
      "[('nacionalidade', 'string')]\n",
      "0\n",
      "--------------------\n",
      "[('renda_anual', 'string')]\n",
      "0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for i in dataset1.columns:\n",
    "    print(f\"{dataset1.select(i).dtypes}\")\n",
    "    print(f\"{dataset1.filter(dataset1[i].isNull()).count()}\")\n",
    "    print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- idade: integer (nullable = true)\n",
      " |-- classe_trabalho: string (nullable = true)\n",
      " |-- final_weight: integer (nullable = true)\n",
      " |-- escolaridade: string (nullable = true)\n",
      " |-- escolaridade_num: integer (nullable = true)\n",
      " |-- estado_civil: string (nullable = true)\n",
      " |-- ocupacao: string (nullable = true)\n",
      " |-- relacionamento_householder: string (nullable = true)\n",
      " |-- raca: string (nullable = true)\n",
      " |-- sexo: string (nullable = true)\n",
      " |-- ganho_capital: integer (nullable = true)\n",
      " |-- perda_capital: integer (nullable = true)\n",
      " |-- jornada_trabalho: integer (nullable = true)\n",
      " |-- nacionalidade: string (nullable = true)\n",
      " |-- renda_anual: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = [\"classe_trabalho\", \"escolaridade\", \"estado_civil\", \n",
    "                       \"ocupacao\", \"relacionamento_householder\", \"raca\",\n",
    "                       \"sexo\", \"nacionalidade\", \"renda_anual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pyspark.sql import SparkSession, DataFrame\n",
    "\n",
    "def remove_accents(categorical_features: List[str], dataset: DataFrame) -> DataFrame:\n",
    "    for categorical_feature in categorical_features:\n",
    "        if categorical_feature != \"renda_anual\":\n",
    "            if categorical_feature == \"nacionalidade\" or categorical_feature == \"classe_trabalho\" or categorical_feature == \"ocupacao\":\n",
    "                dataset = dataset.withColumn(\n",
    "                    categorical_feature,\n",
    "                    regexp_replace(categorical_feature, r'\\?', 'indefinido')  # Using raw string for escaping '?'\n",
    "                )\n",
    "            \n",
    "            # General replacement for all other features\n",
    "            dataset = dataset.withColumn(\n",
    "                categorical_feature,\n",
    "                regexp_replace(categorical_feature, r'[^a-zA-Z0-9]', '')  # Regex to remove non-alphanumeric characters\n",
    "            )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def verify_empty_values(categorical_features: List[str], dataset: DataFrame) -> DataFrame:\n",
    "    for categorical_feature in categorical_features:\n",
    "        for qtd_rows in range(len(dataset.select(categorical_feature).distinct().collect())):\n",
    "            if len(dataset.select(categorical_feature).distinct().collect()[qtd_rows][0]) == 0:\n",
    "                print(f\"Column with string empty: {categorical_feature}\")\n",
    "                print(\"-\"*20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ok = remove_accents(\n",
    "    categorical_features = categorical_features, \n",
    "    dataset = dataset1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_empty_values(\n",
    "    categorical_features = categorical_features, \n",
    "    dataset = dataset_ok\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "def create_ohe_stages(categorical_features, drop_last=False):\n",
    "\n",
    "    stages = []\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        # Define StringIndexer stage\n",
    "        indexer = StringIndexer(\n",
    "            inputCol=feature,\n",
    "            outputCol=f\"{feature}_index\"\n",
    "        )\n",
    "        stages.append(indexer)\n",
    "        \n",
    "        # Define OneHotEncoder stage\n",
    "        encoder = OneHotEncoder(\n",
    "            inputCol=f\"{feature}_index\",\n",
    "            outputCol=f\"{feature}_vec\",\n",
    "            dropLast=drop_last\n",
    "        )\n",
    "        stages.append(encoder)\n",
    "    \n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = [\"idade\", \"final_weight\", \"escolaridade_num\", \n",
    "                      \"ganho_capital\", \"perda_capital\", \"jornada_trabalho\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "ohe_stages = create_ohe_stages(categorical_features)\n",
    "\n",
    "pipeline_ohe = Pipeline(stages=ohe_stages)\n",
    "\n",
    "dataset_transformed = pipeline_ohe.fit(dataset_ok).transform(dataset_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and Test the Clustering strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml import Pipeline\n",
    "    \n",
    "# Define VectorAssembler to combine all features into a single vector\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_vec\" for col in categorical_features] + numerical_features,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "kmeans = KMeans(featuresCol=\"features\", predictionCol=\"prediction\")\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=[assembler, kmeans])\n",
    "\n",
    "# Define the ClusteringEvaluator\n",
    "evaluator = ClusteringEvaluator(\n",
    "    metricName=\"silhouette\", \n",
    "    featuresCol=\"features\", \n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Define parameter grid for KMeans\n",
    "grid = ParamGridBuilder()\\\n",
    "    .addGrid(kmeans.k, [2, 4, 6])\\\n",
    "    .addGrid(kmeans.seed, [1, 20, 40])\\\n",
    "    .build()\n",
    "\n",
    "# Set up CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(dataset_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best k: 2\n"
     ]
    }
   ],
   "source": [
    "# Get the best model and the best K of the Kmeans\n",
    "best_model = cv_model.bestModel\n",
    "best_k = best_model.stages[-1].getK()\n",
    "\n",
    "print(f\"Best k: {best_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best silhouette score: 0.7447907513941895\n"
     ]
    }
   ],
   "source": [
    "# # Extract silhouette score from the cross-validation results\n",
    "avg_metrics = cv_model.avgMetrics\n",
    "best_index = avg_metrics.index(max(avg_metrics))  # Index of the best silhouette score\n",
    "best_silhouette = avg_metrics[best_index]\n",
    "\n",
    "print(f\"Best silhouette score: {best_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset with the best model\n",
    "predictions = best_model.transform(dataset_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_selected = predictions.select(\n",
    "    categorical_features + numerical_features + [\"prediction\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_selected = predictions_selected.withColumnRenamed(\n",
    "    \"prediction\",\n",
    "    \"target\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_selected.createOrReplaceTempView(\"predictions_selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.spark import SparkXGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset for classification process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clustered = spark.sql(\"select * from predictions_selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove Accents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_clustered_ok = remove_accents(\n",
    "    categorical_features = categorical_features, \n",
    "    dataset = dataset_clustered\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Verify empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_empty_values(\n",
    "    categorical_features = categorical_features, \n",
    "    dataset = dataset_clustered_ok\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "ohe_stages = create_ohe_stages(categorical_features)\n",
    "\n",
    "pipeline_ohe = Pipeline(stages=ohe_stages)\n",
    "\n",
    "dataset_transformed = pipeline_ohe.fit(dataset_clustered_ok).transform(dataset_clustered_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[f\"{col}_vec\" for col in categorical_features] + numerical_features,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and test Classification strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n",
      "INFO:XGBoost-PySpark:Running xgboost-2.1.1 on 2 workers with\n",
      "\tbooster params: {'device': 'cpu', 'objective': 'binary:logistic', 'nthread': 1}\n",
      "\ttrain_call_kwargs_params: {'verbose_eval': True, 'num_boost_round': 100}\n",
      "\tdmatrix_kwargs: {'nthread': 1, 'missing': nan}\n",
      "INFO:XGBoost-PySpark:Finished xgboost training!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define the XGBoost Classifier\n",
    "xgb_classification = SparkXGBClassifier(\n",
    "    features_col=\"features\",\n",
    "    label_col=\"target\",\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Define the Pipeline\n",
    "pipeline = Pipeline(\n",
    "    stages=[assembler, xgb_classification]\n",
    ")\n",
    "\n",
    "# Define the evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"target\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Define parameter grid\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(xgb_classification.max_depth, [5, 10])\n",
    "             .addGrid(xgb_classification.n_estimators, [100, 200])\n",
    "             .build())\n",
    "\n",
    "# Define the CrossValidator\n",
    "crossval = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=grid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=10\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "cv_model = crossval.fit(dataset_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model\n",
    "best_model = cv_model.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best accuracy score: 0.9988953754677633\n"
     ]
    }
   ],
   "source": [
    "# Extract accuracy score from the cross-validation results\n",
    "avg_metrics = cv_model.avgMetrics\n",
    "best_index = avg_metrics.index(max(avg_metrics))  # Index of the best accuracy score\n",
    "best_accuracy = avg_metrics[best_index]\n",
    "\n",
    "print(f\"Best accuracy score: {best_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the dataset with the best model\n",
    "predictions = best_model.transform(dataset_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract of the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999692884125181\n"
     ]
    }
   ],
   "source": [
    "performance = MulticlassClassificationEvaluator(labelCol=\"target\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = performance.evaluate(predictions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a TempView"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_renamed = predictions.withColumnRenamed(\n",
    "    \"target\",\n",
    "    \"prediction_kmeans\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_renamed = predictions_renamed.withColumnRenamed(\n",
    "    \"prediction\",\n",
    "    \"prediction_xgboost\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_selected = predictions_renamed.select(\n",
    "    categorical_features + numerical_features + [\"prediction_kmeans\", \"prediction_xgboost\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_selected.createOrReplaceTempView(\"class_predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select of the classification predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-------------------+----------------+--------------------------+----------------+------+-------------+-----------+-----+------------+----------------+-------------+-------------+----------------+-----------------+------------------+\n",
      "|classe_trabalho|escolaridade|       estado_civil|        ocupacao|relacionamento_householder|            raca|  sexo|nacionalidade|renda_anual|idade|final_weight|escolaridade_num|ganho_capital|perda_capital|jornada_trabalho|prediction_kmeans|prediction_xgboost|\n",
      "+---------------+------------+-------------------+----------------+--------------------------+----------------+------+-------------+-----------+-----+------------+----------------+-------------+-------------+----------------+-----------------+------------------+\n",
      "|       Stategov|   Bachelors|       Nevermarried|     Admclerical|               Notinfamily|           White|  Male| UnitedStates|      <=50K|   39|       77516|              13|         2174|            0|              40|                0|               0.0|\n",
      "|  Selfempnotinc|   Bachelors|   Marriedcivspouse|  Execmanagerial|                   Husband|           White|  Male| UnitedStates|      <=50K|   50|       83311|              13|            0|            0|              13|                0|               0.0|\n",
      "|        Private|      HSgrad|           Divorced|Handlerscleaners|               Notinfamily|           White|  Male| UnitedStates|      <=50K|   38|      215646|               9|            0|            0|              40|                0|               0.0|\n",
      "|        Private|        11th|   Marriedcivspouse|Handlerscleaners|                   Husband|           Black|  Male| UnitedStates|      <=50K|   53|      234721|               7|            0|            0|              40|                0|               0.0|\n",
      "|        Private|   Bachelors|   Marriedcivspouse|   Profspecialty|                      Wife|           Black|Female|         Cuba|      <=50K|   28|      338409|              13|            0|            0|              40|                1|               1.0|\n",
      "|        Private|     Masters|   Marriedcivspouse|  Execmanagerial|                      Wife|           White|Female| UnitedStates|      <=50K|   37|      284582|              14|            0|            0|              40|                1|               1.0|\n",
      "|        Private|         9th|Marriedspouseabsent|    Otherservice|               Notinfamily|           Black|Female|      Jamaica|      <=50K|   49|      160187|               5|            0|            0|              16|                0|               0.0|\n",
      "|  Selfempnotinc|      HSgrad|   Marriedcivspouse|  Execmanagerial|                   Husband|           White|  Male| UnitedStates|       >50K|   52|      209642|               9|            0|            0|              45|                0|               0.0|\n",
      "|        Private|     Masters|       Nevermarried|   Profspecialty|               Notinfamily|           White|Female| UnitedStates|       >50K|   31|       45781|              14|        14084|            0|              50|                0|               0.0|\n",
      "|        Private|   Bachelors|   Marriedcivspouse|  Execmanagerial|                   Husband|           White|  Male| UnitedStates|       >50K|   42|      159449|              13|         5178|            0|              40|                0|               0.0|\n",
      "|        Private| Somecollege|   Marriedcivspouse|  Execmanagerial|                   Husband|           Black|  Male| UnitedStates|       >50K|   37|      280464|              10|            0|            0|              80|                1|               1.0|\n",
      "|       Stategov|   Bachelors|   Marriedcivspouse|   Profspecialty|                   Husband|AsianPacIslander|  Male|        India|       >50K|   30|      141297|              13|            0|            0|              40|                0|               0.0|\n",
      "|        Private|   Bachelors|       Nevermarried|     Admclerical|                  Ownchild|           White|Female| UnitedStates|      <=50K|   23|      122272|              13|            0|            0|              30|                0|               0.0|\n",
      "|        Private|   Assocacdm|       Nevermarried|           Sales|               Notinfamily|           Black|  Male| UnitedStates|      <=50K|   32|      205019|              12|            0|            0|              50|                0|               0.0|\n",
      "|        Private|    Assocvoc|   Marriedcivspouse|     Craftrepair|                   Husband|AsianPacIslander|  Male|   indefinido|       >50K|   40|      121772|              11|            0|            0|              40|                0|               0.0|\n",
      "|        Private|      7th8th|   Marriedcivspouse| Transportmoving|                   Husband|AmerIndianEskimo|  Male|       Mexico|      <=50K|   34|      245487|               4|            0|            0|              45|                1|               1.0|\n",
      "|  Selfempnotinc|      HSgrad|       Nevermarried|  Farmingfishing|                  Ownchild|           White|  Male| UnitedStates|      <=50K|   25|      176756|               9|            0|            0|              35|                0|               0.0|\n",
      "|        Private|      HSgrad|       Nevermarried| Machineopinspct|                 Unmarried|           White|  Male| UnitedStates|      <=50K|   32|      186824|               9|            0|            0|              40|                0|               0.0|\n",
      "|        Private|        11th|   Marriedcivspouse|           Sales|                   Husband|           White|  Male| UnitedStates|      <=50K|   38|       28887|               7|            0|            0|              50|                0|               0.0|\n",
      "|  Selfempnotinc|     Masters|           Divorced|  Execmanagerial|                 Unmarried|           White|Female| UnitedStates|       >50K|   43|      292175|              14|            0|            0|              45|                1|               1.0|\n",
      "+---------------+------------+-------------------+----------------+--------------------------+----------------+------+-------------+-----------+-----+------------+----------------+-------------+-------------+----------------+-----------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from class_predictions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop the connection Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
